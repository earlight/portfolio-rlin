---
title: Lecture 13 Blog - Design that Matters
layout: doc
---

# Lecture 13 Blog - Design that Matters

## Responsible Innovation in Artificial Intelligence

In this lecture, Lyel Resner was about to talk about the importance and society-wide benefits of responsible innovation, and how we, as software enigneers, can contribute positively to society in the products that we work on. In recent years, one clear topic of hot debate when it comes to responsible innovation is the use of artificial intelligence, especially with the recent advances in generative models that can produce text, images, and even videos that are becoming increasingly hard to distinguish from real human-generated content.

Responsible innovation in AI requires prioritizing deploying technology that benefits society while minimizing risks like bias, privacy concerns, and misuse. However, achieving this balance may not always be directly benefitial to the companies interests. In lecture, we talked about how the selfish interests of investors may incluence a company's decision, but for AI produces, even consumers may sometimes prefer a product that may potentially be less "safe," with people (many of whom may not be intentionally using AI to produce malicious content) often having the opinon that less "censored" AI generate more entertaining or truthful content. These concerns are why many people speculate that OpenAI's Sora video generator has not been publically released yet due to concerns for misuse before the upcoming US presidential election.

OpenAI's DALL-E 3 image generator was released with lots of gaurdrails and censorship to prevent violent or unsafe content from being generated. While being somewhat safe for widespread use, users have still expressed disappointment in the lack of freedom and creativity that the model has, expecially when generating content for more complex ideas. 

On the other hand, StabilityAI's Stable Diffusion image generator lauched with relatively minimal censorship (for certain versions). While this allowed for more creative freedom of the users, this also received backlash due to some users generating explicit and violent content. Of course, some people argue that the there may be legitimate uses for such content and that users should be the ones to decide and govern what they should and should not generate, but the fact remains that the potential for misuse is still there, and that governing individual users from generating harmful content may be much harder than governing the AI models themselves. 

These are just some of the many examples of the ethical dilemma that AI developers face when creating new generative models. Making choices that protect the public while facing both consumer and investor pressures is a difficult task, where responsible innovation and deep foresight are required to make the right decisions. 

*Note that, ironically, ChatGPT assisted with inspiration for some of the examples in this blog post.*